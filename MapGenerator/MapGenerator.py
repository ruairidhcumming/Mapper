from keras.optimizers import RMSprop
from keras.losses import mean_squared_error
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose,BatchNormalization
from keras.layers import Activation, Dropout, Flatten, Dense,LeakyReLU,UpSampling2D
from keras.applications.vgg16 import preprocess_input
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
import os
import numpy as np
### picture directory as str
picspath = str('C:\\Users\\ruair\\Documents\\rolerball\\screenshotOverhead\\')

###

batch_size = 24

###
print('creating CNN model')

model = Sequential()## lets try 3 conv layers down then conv transpose  three up again
model.add(Conv2D(32, (3, 3), input_shape=( 150, 150,3)))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))

model.add(Conv2DTranspose(32, kernel_size=3, strides=2, padding='same'))
#model.add(BatchNormalization())
#model.add(LeakyReLU(alpha=0.01))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))


##now conv  and upsample back to 150,150,3
model.add(UpSampling2D())
#model.add(Conv2DTranspose(32, kernel_size=5, strides=2, padding='same'))
#model.add(BatchNormalization())
#model.add(LeakyReLU(alpha=0.01))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))


model.add(Conv2D(3,(3,3)))

model.add(Activation('relu'))

#model.add(Conv2DTranspose(32, kernel_size=10, strides=2, padding='same'))
#model.add(BatchNormalization())
#model.add(LeakyReLU(alpha=0.01))
model.summary()
##training data generator to stream images out of directort and split ( left half = x , right half = y)
#train_generator = train_datagen.flow_from_directory(picspath,
#                                                    target_size = (150,150), class_mode = None)
def random_crop(img, random_crop_size):## stolen from https://jkjung-avt.github.io/keras-image-cropping/
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx), :]
def lefthalf_crop(img):## stolen from https://jkjung-avt.github.io/keras-image-cropping/
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    x=round(width/2)
    return img[0:height, 0:x, :]
def righthalf_crop(img):## stolen from https://jkjung-avt.github.io/keras-image-cropping/
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]

    x = round((width/2))
    return img[0:y, x:width, :]
def crop_generator(batches):## stolen from https://jkjung-avt.github.io/keras-image-cropping/
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    while True:
        print (batches[1].shape)
        batch_x = next(batches)
        

        batch_y_out = np.zeros((batch_x.shape[0], batch_x.shape[0], round(batch_x.shape[1]/2),3))
        batch_x_out = np.zeros((batch_x.shape[0], batch_x.shape[0], round(batch_x.shape[1]/2),3))
        for i in range(batch_x.shape[0]):
            batch_y_out[i] = righthalf_crop(batch_x[i])
            batch_x_out[i] = lefthalf_crop(batch_x[i])
        yield (batch_x_out, batch_y_out)
##example data flow for cropper
#train_datagen = ImageDataGenerator(......)
#train_batches = train_datagen.flow_from_directory(DATASET_PATH + '/train',
#                                                  target_size=(256,256),
#                                                  ......)
#train_crops = crop_generator(train_batches, 224)
#
### load data and separate 
## dont use this it doesnt work 
##train_datagen = ImageDataGenerator(rescale=1./255)
#train_batches = train_datagen.flow_from_directory(picspath, target_size=(300,150), batch_size = batch_size)
#train_x,train_y = crop_generator(train_batches)

fileList = os.listdir(picspath)     

#def imageLoader(files, batch_size):

#    L = len(files)

    #this line is just to make the generator infinite, keras needs that    
 #while True:

  #      batch_start = 0
   #     batch_end = batch_size

    #    while batch_start < L:
     #       limit = min(batch_end, L)
      #      images = load_img(files[batch_start:limit])
       #     X = lefthalf_crop(images)
        #    Y= righthalf_crop(images)

         #   yield (X,Y) #a tuple with two numpy arrays with batch_size samples     

          #  batch_start += batch_size   
           # batch_end += batch_size




def read_image(path):
    img = load_img(path, target_size=( 300, 150))
    tmp = img_to_array(img)
    tmp = np.expand_dims(tmp, axis=0)
    tmp = preprocess_input(tmp)
    tmp=tmp*1./225
    return tmp
batch_holder = np.zeros((batch_size, 300, 150, 3))
for j in range(0,batch_size-1):
    batch_holder[j, :] = read_image(picspath+fileList[j])

def image_batcher (dir, imagelist,batch_size):
    batch_holder = np.zeros((batch_size, 300, 150, 3))
    for j in range(0,batch_size-1):
        batch_holder[j, :] = read_image(dir+imagelist[j])
    return (batch_holder)

trainX = batch_holder[:,0:150,:,:]
trainY = batch_holder[:,0:118,0:118,:]##stupid size consraint grr
images = image_batcher(picspath,fileList,batch_size)
trainX = images[:,0:150,:,:]
trainY = images[:,0:118,0:118,:]

model.compile(optimizer=RMSprop(), loss = 'mse'  )
model.fit(trainX,trainY, batch_size = batch_size,epochs = 20)  
testimage = image_batcher(picspath,fileList,1)[:,0:150,:,:]
ans = model.predict(testimage)
ans = array_to_img(ans)
ans.show()